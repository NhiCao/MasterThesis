\documentclass[12pt]{article}

\usepackage{naist-jmthesis}
%\usepackage{naist-mthesis}
\usepackage[dvipdfmx]{graphicx}
\usepackage{slashbox}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{susumu2e}
\usepackage{color}
\usepackage{amssymb}
\usepackage{amsxtra}
\usepackage{threeparttable}
\usepackage{tabularx}
\usepackage{algorithm,algorithmic}
\usepackage{cite}

\newcommand{\argmin}{\mathop{\rm argmin}\limits}

\pagestyle{final}
\lang{English}
\studentnumber{6612180092-7}
\doctitle{\mastersthesis}
\major{\engineering}
\title{Data Evaluation for Improving TTS Voices Trained on Crowdsourced Datasets}
\author{CAO Hai Nhi}
% CHANGE year XXXXX
\syear{2020}
\smonth{8}
\sday{4}
%

% 審査委員（日本語）
%   （姓と名，名と称号の間に空白を入れて下さい）
\ecmembers{Professor Yoichi Yamashita}{}{}{}			% ２人の場合
%
\etitle{Data Evaluation for Improving TTS Voices Trained on Crowdsourced Datasets}
\eauthor{CAO Hai Nhi}
% キーワード５-６個 (in LaTeX)
\ekeywords{speech synthesis, data evaluation, crowdsourced datasets}
%
% 内容梗概 (in LaTeX)

% 1ページに収めることが望ましい
\eabstract{
Usually, to synthesize a high-quality voice, it is required to have a dataset which is recorded by a professional speaker using high-quality equipment under carefully controlled conditions. The creation of such a dataset costs a lot of time, effort, and expense. In order to reduce the cost, my research tries to use a crowdsourced dataset which is created by many non-professional speakers recording their voices and contributing via Internet. This data is often recorded in normal conditions using low-cost devices such as mobile phones and laptop microphones, so it often suffers from containing noise, bad pronunciation, and inconsistent settings. My research deals with these problems by evaluating the data and processing the dataset before using it to train speech synthesis system. The ultimate goal is to improve the quality of voices synthesized from such low-cost datasets.

In our work, we aim to address three problems. First, we investigate what factors affect the quality of synthetic voices. Second, we evaluate the quality of the data based on these factors. Third, we perform appropriate processings on the dataset based on the evaluation in order to improve the quality of the synthetic voice.

We find that speech power, signal-to-noise ratio and transcribed noise affect the quality of synthetic voices. Evaluating and performing appropriate processings on the dataset using these three factors can significantly improve the quality of the synthetic voice.
}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%% document starts here %%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% 表紙
\titlepage
% 審査委員ページ
\cmemberspage
% 内容梗概
\firstabstract
%
% 目次
%
\toc
\newpage
\listoffigures

%\newpage
\listoftables
%
% これ以降本文
%
\newpage
% 頁番号をアラビア数字に変更
\pagenumbering{arabic}

%-----------------------------------------------------------------
% INTRODUCTION
%-----------------------------------------------------------------

\section{Introduction}
Speech synthesis or text-to-speech (TTS) is a technique that converts written text into artificial speech. This technique has been rapidly developed in the recent years and has already reached a level of almost as natural as human speech. However, there are still some problems remaining such as great resource consuming and costly datasets. My research aims to reduce the cost by using cheaper datasets.

Usually, speech synthesis systems require datasets that consume a great deal of time, effort, and expense to create. Traditional TTS datasets are recorded by professional speakers who possess correct and clear pronunciation. The amount of speech data needed to train a speech synthesis system varies from several hours to tens of hours. Usually, the more data, the more intelligible and natural the synthetic voice. This means that to produce a high-quality synthetic voice, the speaker has to speak tens of hours with high quality of his/her speaking and maintain so throughout the recording process. Moreover, the recording must be carried out in a quiet and anechoic chamber using high-quality devices.

With the goal to recude the cost in creating TTS datasets, many researches use other sources of speech data that are available online such as audiobooks, broadcast news and ASR datasets. These research showed the effectiveness of several techniques that help improve the quality of voices synthesized from those kinds of found data. Crowdsourced datasets, however, have not been investigated yet.

Recently, with the rapid development of Internet, it becomes easier for people to get, share and contribute information on a social platform. Speech data collection, as a result, becomes easier and cheaper. Crowdsourced datasets are created by many people contributing their voices via Internet. A dataset of this kind contains data of many speakers who may be professional or non-professional. This leads to the dataset may contain recordings with bad pronunciation. Also because of various speakers, recording conditions as well as speaking styles are different. These issues need to be tackled before using the datasets in synthesizing speech.

In this research, we focus on investigating what factors affect the quality of synthetic voices and using them as criteria to evaluate the quality of crowdsourced datasets. Data cleaning is performed after the evaluation. Both objective and subjective evaluation are conducted to evaluate the quality of the voices trained from the cleaned datasets in order to confirm the effectiveness of the approach.

The structure of this thesis is as follows. Section 2 introduces factors that we hypothesize might affect the quality of synthetic voices. Section 3 gives a brief introduction about the tools, datasets and speech synthesis method used in our work. Section 4 presents the investigations conducted for each of the factors to see which ones help improve the quality of synthetic voices. In Section 5, we present experiments of evaluating and performing data cleaning using the effective factors. Finally, chapter 6 concludes our findings and discusses some ideas for future work.
 
%-----------------------------------------------------------------
% POTENTIAL FACTORS AFFECTING THE QUALITY OF SYNTHETIC VOICES
%-----------------------------------------------------------------

\clearpage
\section{Potential Factors Affecting the Quality of Synthetic Voices}\label{sec_potentialFactors}
To evaluate the quality of data used to train speech synthesis systems, we need to know what factors affect the quality of synthetic voices. The below factors are derived from characteristics of traditional TTS datasets that we hypothesize might contribute to the intelligibility and naturalness of synthetic voices.

\subsection{Speech power}\label{subsec_speechPower}
The power of a signal is the sum of the absolute squares of its time-domain samples divided by the signal length. The power in signal $x(n)$ is
\begin{equation}\label{exp_power}
    P = \lim_{N\to\infty} (\frac{1}{2N+1}\sum_{n=-N}^{N} |x(n)|^2),
\end{equation}
where $N$ is the signal length.

In traditional TTS datasets, speakers are usually intructed to speak with a moderate loud voice and try to keep the same loudness during the entire recording process. Crowdsourced datasets, however, are recorded by many people with many recording conditions, so it is impossible to have a consistent volume for all recordings, if not to say they vary greatly. Because of this, we decide to investigate speech power in speech recordings to see if it has any effect on the quality of synthetic voices.

\subsection{Signal-to-noise ratio}
Signal-to-noise ratio (SNR) is defined as the ratio of signal power to the noise power, often expressed in decibels (dB). SNR is computed as
\begin{equation}\label{exp_SNR}
    SNR = \frac{P_{signal}}{P_{noise}},
\end{equation}
where $P_{signal}$ and $P_{noise}$ are the signal power and the noise power, respectively. A ratio higher than 1:1 (greater than 0 dB) indicates more signal than noise.

Usually, recordings used for speech synthesis are recorded in a studio with minimum environmental noise and reverberation. As a result, SNRs of these recordings are very high. Meanwhile, SNRs in many recordings in crowdsourced datasets may be low or very low due to bad recording conditions.
We hypothesize that noisy or low SNR recordings can significantly degrade the intelligibility of synthetic voices.

\subsection{Standard deviation of F0}
Standard deviation of F0 (henceforth referred as stdF0) is used to assess the variation of F0 in a recording. stdF0 of a recording describes how much the speaker changes their pitch while speaking. According to [6], speakers in TTS datasets are instructed to speak in their most natural and comfortable style without varying that style significantly. High stdF0 is due to expressive speech or inconsistent speaking style which are likely to happen in crowdsourced datasets where speech data comes from many speakers and the speakers themselves are not professionals.

\subsection{Transcribed noise}
Transcribed noise is used to assess the inconsistency between speech and transcript. High transcribed noise might due to poor transcription or bad pronunciation. Because speakers are not professionals, the chances of making mistakes at pronunciation in crowsourced datasets are higher than in TTS ones. It is obvious that the inconsistency between speech and transcript will strongly affect the quality of synthetic voices.

%-----------------------------------------------------------------
% SPEECH SYNTHESIS METHOD, TOOLS, AND DATASETS
%-----------------------------------------------------------------

\clearpage
\section{Speech Synthesis Method, Tools, and Datasets}\label{sec_methodToolsDatasets}

\subsection{Speech synthesis method}
Statistical parametric speech synthesis has been shown to be more effective in generating synthetic speech compared to the traditional method of concatenative synthesis in many aspects. Statistical parametric approaches producing speech parameters based on statistical models instead of concatenating speech units makes voice characteristics easier to modify. Concatenative synthesis requires that the entire corpus be spoken by the same speaker while parametric synthesis learns average statistics about what speech sounds like, thus, we can train voices on data that contains many speakers, recording conditions, and speaking styles [1]. This makes statistical parametric speech synthesis more appropriate for synthesizing voices from crowdsourced datasets. In our work, we use CLUSTERGEN synthesizer which is a statistical parametric speech synthesis method implemented within the Festival/FestVox voice building environment [2].

CLUSTERGEN synthesizer is divided into two parts, training and synthesis. However, before the training part can take place, a step of labeling data is required. Labeling data is to align the phonemes generated from the transcripts with the audios. This process is done by using an HMM labeler with a 3-state HMM model built for each phone.

In the training part, acoustic and linguistic features are extracted from transcript and audio signal, respectively. Linguistic features are represented as sequences of phonemes in context. Acoustic features are represented as vectors consisting of F0 and spectrum parameters (MFCC). The training is a clustering process which is done by using CART trees. CART trees are built to find questions that split the data into clusters with minimum impurity. The clusters are optimized to minimize the sum of the standard deviations of each MFCC feature multiplied by the number of samples in the cluster. A tree is built for all the vectors labeled with the same HMM state name.

At synthesis time, a phoneme sequence is generated from the text, then an HMM state name relation is built linking each phone to its three subphonetic parts. Using the CART tree specific to the state name, the questions are asked and the means from the vector at the selected leaf are added as values to each vector. Then the speech is reconstructed from the predicted parameters using MLSA filter.

\subsection{Tools}

\subsubsection{Festival Speech Synthesis}
Festival/FestVox [3] serves as a general framework for building speech synthesis systems. It offers a full text to speech system with various APIs, as well as an environment for development and research of speech synthesis techniques. CLUSTERGEN synthesizer is a new synthesis technique added to the FestVox suite of voice building tools [4]. Festival/FestVox allows users to synthesize speech in multiple languages such as English (British and American), and Spanish. It is a free software and includes a step-by-step tutorial with examples in a document called “Building Synthetic Voices”.

\subsubsection{Signal-to-noise ratio estimation}
To estimate the level of noise or signal-to-noise ratio (SNR) in speech recordings, we use a tool from [N]. This is a collection of Matlab functions calculating SNR using different methods. Among these methods, WADA is the one we use for our work. WADA SNR can calculate SNR without requiring a reference signal which is appropriate for our research because with crowdsourced datasets, we can not have the reference signals (clean speech recordings), only noisy recordings are available. Among the methods being able to calculate SNR without requiring reference signal, WADA is the one giving the most accurate results.

\subsubsection{F0 estimation}
Praat [5] is a free computer software package for speech analysis in phonetics. It offers a wide range of functions for processing speech, including pitch analysis. Praat is well documented and used widely in the field of speech signal processing. Many research used Praat to extract speech features or as reference methods.

\subsection{Datasets}

\subsubsection{TTS datasets}
The CMU\_ARCTIC databases were constructed at the Language Technologies Institute at Carnegie Mellon University as phonetically balanced, US English single speaker databases designed for unit selection speech synthesis research. The databases consist of approximately one hour per speaker. We pick two of these databases, “cmu\_us\_rms\_arctic” and “cmu\_us\_clb\_arctic”, to use in our investigation. The first one is of a male speaker and the second one is of a female speaker.

The CMU\_INDIC databases were constructed at the Language Technologies Institute at Carnegie Mellon University as phonetically balanced, single speaker databases designed for corpus based speech synthesis research. They are covering major languages spoken in the Indian subcontinet. We also pick two of these databases, “cmu\_indic\_guj\_ad” and “cmu\_indic\_ben\_rm” to use in our investigation. The former is of a male speaker and contains about 1000 recordings. The latter is of a female speaker and contains about 500 recordings.

JSUT (Japanese speech corpus of Saruwatari-lab., University of Tokyo) corpus is a Japanese corpus consisting of transcription and reading-style audios. The audio data is recorded in an anechoic room by a native Japanese female speaker. The whole corpus contains 10-hour speech divided into many subsets. We only use one subset named “basic5000” for our investigation. This subset contains 5000 recordings.

JVS (Japanese versatile speech) corpus consists of Japanese text (transcripts) and multi-speaker voice data. The speech data is recorded in a studio by professional speakers. We use two subsets, jvs001 and jvs002, of two speakers, one male and one female. Each subset contains 100 recordings.

SWARA - Mobile System for Rehabilitative Vocal Assistance of Surgical Aphonia is one of the largest Romanian speech datasets freely available for both academic and commercial use. SWARA is a national project with the main objective of enabling speech impaired persons to use a fast, personalised text-to-speech synthesis system. The corpus amounts to approximately 21 hours of high-quality read speech data. We use two subsets of this corpus which are BAS (female speaker) and FDS (male speaker).

The SIWIS French Speech Synthesis Database includes high quality French speech recordings and associated text files, aimed at building TTS systems. A total of 9750 utterances from various sources such as parliament debates and novels were uttered by a professional French voice talent. The database is divided into 5 parts. Part 1 which contains 4500 recordings is used in our investigation.

Arabic Speech Corpus Speech corpus was developed as part of PhD work carried out by Nawar Halabi at the University of Southampton. The corpus contains 1813 recordings recorded in south Levantine Arabic (Damascian accent) using a professional studio. The number of recordings is not big, so we use the whole corpus in our investigation.

\subsubsection{Crowdsourced dataset}

Common Voice is a crowdsourcing project started by Mozilla to create a free database for speech recognition software. The project is supported by volunteers who record sample sentences with a microphone and review recordings of other users. This is a massively-multilingual collection of transcribed speech recorded by over 50,000 individuals, resulting in 2,500 hours of collected audio. In our work, we only use recordings of English with US accent. Different subsets of the dataset are used in different investigations depending on the investigated factors.


%-----------------------------------------------------------------
% INVESTIGATING THE EFFECT OF THE FACTORS ON THE QUALITY OF SYNTHETIC VOICES
%-----------------------------------------------------------------

\clearpage
\section{Investigating the Effect of the Factors on the Quality of Synthetic Voices}\label{sec_investigations}
This section presents four investigations corresponding to the four potential factors listed in section {\ref{sec_potentialFactors}}. The investigations aim to determine which factors affect the quality of synthetic voices.

\subsection{Speech power}

We started off the investigation of speech power by examining its distribution in a crowdsourced dataset. Because examining power distribution in crowdsourced dataset only did not provide much information, we compared it with ones in TTS datasets in order to observe the difference between them. The difference helped us devise policies to process the data so that the new data can produce voice with higher quality.

Common Voice is a very large dataset containing 2500 hours of speech. Because we did not have enough time and resource to train voice using the whole dataset, we used only the first 2000 recordings of US accent for this investigation of speech power.

To calculate power distribution of each dataset, we first calculated speech power of each recording using formula \ref{exp_power} and then calculated the mean and standard deviation of power for the whole dataset. The power distribution of each dataset (crowdsourced and TTS) is shown in Table \ref{tab_powerDistribution} and Figure \ref{fig_powerDistribution}.

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.5]{image/powerDistributionInDatasets}
\end{center}
\vspace{-0.3cm}
\caption[Power distribution in TTS and crowdsourced datasets.]{Power distribution of TTS and crowdsourced datasets.}
\label{fig_powerDistribution}
% \vspace{-0.3cm}
\end{figure}

\begin{table}[]
\begin{center}
\caption{Power mean and standard deviation of TTS and crowdsourced datasets.}
\label{tab_powerDistribution}
\vspace{3mm}
\begin{tabular}{lcc}
\hline
Dataset & Power mean & Power standard deviation \\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}\cmidrule(l{0pt}r{2pt}){3-3}
arcticRms   & 0.0058 & 0.0018 \\
arcticClb   & 0.0193 & 0.0049 \\
indicBen    & 0.0089 & 0.0031 \\
indicGuj    & 0.0092 & 0.0028 \\
jsutBasic   & 0.0028 & 0.000196 \\
jvs001      & 0.0022 & 0.00052777 \\
jvs002      & 0.0014 & 0.00036044 \\
swaraBas    & 0.0278 & 0.0084 \\
swaraFds    & 0.0207 & 0.0069 \\
siwis       & 0.0093 & 0.0026 \\
arabic      & 0.0238 & 0.0069 \\
commonVoice & 0.0053 & 0.0111 \\
\hline
\end{tabular}
\end{center}
\end{table}

From Figure \ref{fig_powerDistribution}, it can be seen that the mean power of crowdsourced dataset is acceptable compared to the other ones but its power variation is very much higher. Therefore, we decided to normalize its power to see if this helps improve the quality of synthetic voices. The power normalization was performed by bringing power of all recordings in the dataset to the mean power. For each recording, the power was increased or decreased depending on it was lower or higher than the mean, respectively. Figure \ref{fig_normalizingPower} shows the steps of performing power normalization.
\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/flowChartNormalizingPower}
\end{center}
\vspace{-0.3cm}
\caption[performing power normalization.]{Flow chart of performing power normalization.}
\label{fig_normalizingPower}
% \vspace{-0.3cm}
\end{figure}
The dataset created after the normalization has a new power distribution with mean of 0.0053 and standard deviation of $4.0554e-5$. Notice that the new standard deviation is almost 0.

To see how the power normalization affects the quality of synthetic voice, we evaluated the voices synthesized from the dataset before and after the normalization by using both objective and subjective evaluation. The synthetic voices were named $voice\_original$ and $voice\_powerNormalized$, respectively.\\\\
\textbf{Objective evaluation}
\vspace{0.28cm}\\
Objective evaluation aiming to evaluate the intelligibility of the voices was conducted by using speech recognition APIs to recognize the synthetic speech. We used two APIs for this task which were wit.ai, a natural language API toolkit owned by Facebook, and Watson, an IBM’s API for cognitive applications.

The objective evaluation was conducted as follows. First, we synthesized 20 utterances for each voice. The sentences used for synthesizing utterances come from four different domains: novel, news, conversation and scientific papers (5 sentences per each domain, the list of the sentences can be found in appendix). The synthesized utterances were then recognized by the two APIs. After utterances were recognized, we computed word error rate (WER) for each voice. The WER of a voice was an average over the 20 WERs of the 20 utterances. WER of an utterance was computed as
\begin{equation}\label{exp_wer}
    WER = \frac{Substitutions + Insertions + Deletions}{Number\:of\:Words\:Spoken}.
\end{equation}
The intelligibility of each voice was assessed based on its WER. WERs of the two synthetic voices are shown in Table \ref{tab_werPower}.

\begin{table}[]
\begin{center}
\caption{WER of synthetic voices before and after performing power normalization.}
\label{tab_werPower}
\vspace{3mm}
\begin{tabular}{lcc}
\hline
Voice & WER – Watson (\%) & WER – witai (\%) \\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}\cmidrule(l{0pt}r{2pt}){3-3}
voice\_original          & 68.15          & 82.08 \\
voice\_powerNormalized   & \textbf{60.53} & \textbf{75.51} \\
\hline
\end{tabular}
\end{center}
\end{table}

As we can see from the Table, WERs of the voice synthesized from the dataset after performing power normalization are lower than the one synthesized from the original dataset in both APIs. This means that performing power normalization helped improve the intelligibility of the synthetic voice.\\\\
\textbf{Subjective evaluation}
\vspace{0.28cm}\\
Because the cost of conducting subjective evaluation is high, we reduced the number of sentences used to synthesize utterances from 20 to 8. However, the sentences were also from the four domains mentioned above (2 sentences per each domain) and were a subset of the 20 sentences. The evaluation was designed as pairwise comparisons between the original voice and the voice after performing power normalization. There were 8 pairs of synthesized utterances corresponding to the 8 sentences. The order of the voices in each pair was shuffled to avoid possible order effects. Evaluators were asked to listen to the two utterances in each pair and then decided which one had higher quality. Neutral answers were acceptable. Table \ref{tab_subEvaPower} presents the evaluation results from 12 evaluators.

\begin{table}[]
\begin{center}
\caption{Subjective evaluation of synthetic voices before and after performing power normalization.}
\label{tab_subEvaPower}
\vspace{3mm}
\begin{tabular}{lc}
\hline
Voice & Preference score (\%) \\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}
voice\_original         & 26.04 \\
voice\_powerNormalized  & \textbf{54.17} \\
No preference           & 19.79 \\
\hline
\end{tabular}
\end{center}
\end{table}

Results of subjective evaluation shows that the voice synthesized from the dataset after performing power normalization was preferred to the voice synthesized from the original dataset.

From both objective and subjective evaluation, we conclude that power normalization does help improve the quality of synthetic voices.

\subsection{Signal-to-noise ratio (SNR)}
To see if SNR affects the quality of synthetic voices, we increasingly removed 5\%, 10\%, …, 25\% of recordings which had lowest SNRs (the noisiest ones) from the two datasets (subsets) and then compared the voices synthesized from the datasets created after removing against the voices synthesized from the original datasets.

Once again, because Common Voice is a very large dataset with about 2500 hours of speech, we only used small subsets of it for investigation. In this investigation of SNR, we used two subsets which consist of 2000 and 4000 recordings from the dataset. There were no overlaps between these two subsets. The investigation was conducted on two datasets to increase the reliability of the results.

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/snrHist2000}
\end{center}
\vspace{-0.3cm}
\caption[SNR Hist 2000.]{Histogram of SNR in 2000-recording dataset.}
\label{fig_snrHist2000}
% \vspace{-0.3cm}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/snrHist4000}
\end{center}
\vspace{-0.3cm}
\caption[SNR Hist 2000.]{Histogram of SNR in 4000-recording dataset.}
\label{fig_snrHist4000}
% \vspace{-0.3cm}
\end{figure}

First, in the 2000-recording dataset, we calculated SNR value for each recording and then sorted the list of recordings from low to high. We increasingly removed 5\%, 10\%, …, 25\% of recordings which had lowest SNRs from that list. For each amount of removal, a new dataset was created. After that, six voices were synthesized, one from the original dataset, and five from the newly created ones. The same process was done for the 4000-recording dataset. Histograms of SNR in the two original datasets with the amounts of removed data are shown in Figure \ref{fig_snrHist2000} and \ref{fig_snrHist4000}.\\\\
\textbf{Objective evaluation}
\vspace{0.28cm}\\
Objective evaluation was similarly conducted as in the power investigation except one more step on processing the WERs from the two APIs of each voice, namely computing the average of the 2 WERs as in the following formula:
\begin{equation}\label{exp_averageWer}
    WER = \frac{WER_{Watson} + WER_{witai}}{2}.
\end{equation}
The purpose of averaging was to make it easier when comparing among the voices. Objective evaluation results are shown in Table \ref{tab_werSnr2000} and \ref{tab_werSnr4000} correspoding to 2000-recording and 4000-recording dataset. Notice that the two voices synthesized from the two original datasets are called baseline voices.

\begin{table}[]
\begin{center}
\caption{WER of voices synthesized from 2000-recording dataset before and after removing lowest SNR recordings.}
\label{tab_werSnr2000}
\vspace{3mm}
\begin{tabular}{lccc}
\hline
Voice & WER – Watson (\%) & WER – witai (\%) & WER – average (\%)\\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}\cmidrule(l{0pt}r{2pt}){3-3}\cmidrule(l{0pt}r{2pt}){4-4}
Baseline          & 86.81 & 90.89 & 88.85 \\
After remove 5\%  & 76.55 &	92.50 & 84.53 \\
After remove 10\% & 73.89 & 81.41 & 77.65 \\
After remove 15\% & 84.66 & 87.35 & 86.01 \\
After remove 20\% & 84.23 & 87.62 & 85.93 \\
After remove 25\% & 79.17 & 83.29 & 81.23 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[]
\begin{center}
\caption{WER of voices synthesized from 4000-recording dataset before and after removing lowest SNR recordings.}
\label{tab_werSnr4000}
\vspace{3mm}
\begin{tabular}{lccc}
\hline
Voice & WER – Watson (\%) & WER – witai (\%) & WER – average (\%)\\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}\cmidrule(l{0pt}r{2pt}){3-3}\cmidrule(l{0pt}r{2pt}){4-4}
Baseline          & 67.77 & 78.81 & 73.29 \\
After remove 5\%  & 59.52 & 75.29 & 67.41 \\
After remove 10\% & 62.58 & 72.41 & 67.50 \\
After remove 15\% & 57.36 & 71.91 & 64.64 \\
After remove 20\% & 57.11 & 72.30 & 64.71 \\
After remove 25\% & 55.82 & 71.57 & 63.70 \\
\hline
\end{tabular}
\end{center}
\end{table}

In the tables, the values in bold are the better ones compared to the baselines. A voice that has average WER lower than its baseline means it is more intelligible. We can see that, after removing recordings with lowest SNRs, the synthetic voices almost completely outperformed the baseline ones in both datasets. In other words, removing low SNR recordings did help improve the intelligibility of synthetic voices.\\\\
\textbf{Subjective evaluation}
\vspace{0.28cm}\\
Because subjective evaluation is expensive, we cannot compare all the 5 voices against the baseline. If we do so, there will be 5 pairs of voices to compare. Instead, only the voice which performed best in objective evaluation (the one with highest intelligibility or lowest WER) was chosen to compare with the baseline. So, in 2000-recording dataset, the baseline was compared with the voice synthesized after removing 10\% and in 4000-recording dataset, the one synthesized after removing 25\% was chosen to be evaluated against its baseline.

Subjective evaluation was also similarly conducted as in the power investigation except this time, we used 12 sentences (instead of 8) which were, again, a subset of the 20 sentences used in objective evaluation. Results from 22 evaluators are shown in Table \ref{tab_subEvaSnr}.

\begin{table}[]
\begin{center}
\caption{Preference score of voices synthesized from 2000- and 4000-recording datasets before and after removing low SNR recordings.}
\label{tab_subEvaSnr}
\vspace{3mm}
\begin{tabular}{lcc}
\hline
Voice & 2000-recording dataset & 4000-recording dataset \\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}\cmidrule(l{0pt}r{2pt}){3-3}
Baseline          & 31.06\% & 34.85\% \\
After removing low SNR recordings  & \textbf{39.39\%} & \textbf{40.53\%} \\
No preference & 29.55\% & 24.62\% \\
\hline
\end{tabular}
\end{center}
\end{table}

From the results of subjective evaluation, we can see that the voice synthesized after removing low SNR recordings was preferred over its baseline in both datasets.

From both objective and subjective evaluation, we conclude that removing low SNR recordings helps improve the quality of synthetic voices.

\subsection{Standard deviation of F0 (stdF0)}
stdF0 of a recording is the variation of F0 in that recording. High stdF0 is due to expressive speech or inconsistent speaking style which we hypothesize might hurt the quality of synthetic voices. The idea of conducting stdF0 investigation is the same as in the SNR investigation. We also increasingly removed 5\%, 10\%, …, 25\% of the data with highest stdF0 and then compared the voices synthesized from the new datasets against the voice synthesized from the original one. This investigation was also conducted using two subsets consisting of 2000 and 4000 recordings extracted from Common Voice dataset. Histograms of stdF0 in the two datasets (subsets) with the amounts of removed data are shown in Figure \ref{fig_stdF0Hist2000} and \ref{fig_stdF0Hist4000}.

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/stdF0Hist2000}
\end{center}
\vspace{-0.3cm}
\caption[stdF0 Hist 2000.]{Histogram of stdF0 in 2000-recording dataset.}
\label{fig_stdF0Hist2000}
% \vspace{-0.3cm}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/stdF0Hist4000}
\end{center}
\vspace{-0.3cm}
\caption[stdF0 Hist 4000.]{Histogram of stdF0 in 4000-recording dataset.}
\label{fig_stdF0Hist4000}
% \vspace{-0.3cm}
\end{figure}

After the voices were synthesized, we conducted objective and subjective evaluation to evaluate the quality of them using the same procedures as in SNR investigation.\\\\
\textbf{Objective evaluation}
\vspace{0.28cm}\\
Objective evaluation results are shown in Table \ref{tab_werStdF02000} and \ref{tab_werStdF04000}.

\begin{table}[]
\begin{center}
\caption{WER of voices synthesized from 2000-recording dataset before and after removing highest stdF0 recordings.}
\label{tab_werStdF02000}
\vspace{3mm}
\begin{tabular}{lccc}
\hline
Voice & WER – Watson (\%) & WER – witai (\%) & WER – average (\%)\\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}\cmidrule(l{0pt}r{2pt}){3-3}\cmidrule(l{0pt}r{2pt}){4-4}
Baseline          & 65.54 & 76.37 & 70.96 \\
After remove 5\%  & 58.74 & 73.69 & 66.22 \\
After remove 10\% & 61.61 & 68.60 & 65.11 \\
After remove 15\% & 64.12 & 73.48 & 68.80 \\
After remove 20\% & 61.57 & 69.17 & 65.37 \\
After remove 25\% & 63.14 & 72.39 & 67.77 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[]
\begin{center}
\caption{WER of voices synthesized from 4000-recording dataset before and after removing highest stdF0 recordings.}
\label{tab_werStdF04000}
\vspace{3mm}
\begin{tabular}{lccc}
\hline
Voice & WER – Watson (\%) & WER – witai (\%) & WER – average (\%)\\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}\cmidrule(l{0pt}r{2pt}){3-3}\cmidrule(l{0pt}r{2pt}){4-4}
Baseline          & 62.89 & 78.58 & 70.74 \\
After remove 5\%  & 64.08 & 73.57 & 68.83 \\
After remove 10\% & 59.85 & 76.46 & 68.16 \\
After remove 15\% & 58.37 & 78.16 & 68.27 \\
After remove 20\% & 59.63 & 72.07 & 65.85 \\
After remove 25\% & 62.95 & 75.93 & 69.44 \\
\hline
\end{tabular}
\end{center}
\end{table}

The results show that removing high stdF0 recordings helped improve the intelligibility of synthetic voices in both datasets.\\\\
\textbf{Subjective evaluation}
\vspace{0.28cm}\\
In subjective evaluation, two pairs of voices were evaluated. The first one coming from the 2000-recording dataset consisted of the baseline and the voice synthesized after removing 10\% (the best voice chosen from objective evaluation). The second one coming from the 4000-recording dataset consisted of the baseline and the voice synthesized after removing 20\% (the best voice chosen from objective evaluation). Subjective evaluation results from 25 evaluators are shown in Table \ref{tab_subEvaStsF0}.

\begin{table}[]
\begin{center}
\caption{Preference score of voices synthesized from 2000- and 4000-recording datasets before and after removing high stdF0 recordings.}
\label{tab_subEvaStsF0}
\vspace{3mm}
\begin{tabular}{lcc}
\hline
Voice & 2000-recording dataset & 4000-recording dataset \\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}\cmidrule(l{0pt}r{2pt}){3-3}
Baseline & 32.33\% & 33.67\% \\
After removing high stdF0 recordings & 30.67\% & 30.67\% \\
No preference & 37.00\% & 35.66\% \\
\hline
\end{tabular}
\end{center}
\end{table}

Unlike the SNR investigation, the voices synthesized after removing high stdF0 recordings were not preferred to the baselines.

From objective and subjective evaluation, we conclude that removing high stdF0 recordings helps improve the intelligibility of synthetic voices but not the overall quality.

\subsection{Transcribed noise}
Transcribed noise is the inconsistency between speech and transcript. To detect recordings containing transcribed noise in a dataset, we used a speech recognition API to recognize all recordings in that dataset and then calculated WER for each recording using its transcript and the recognized text. Recordings with high WER were considered to have high transcribed noise. The API used to recognized recordings was Watson.

The same approach was applied for this investigation. We removed increasing amounts of recordings with highest WERs from two datasets which were two subsets of 2000 and 4000 recordings from Common Voice dataset. From each new dataset, a voice was synthesized and compared to its corresponding baseline. The comparisons were made based on both objective and subjective evaluation. Histograms of WER in the two datasets (subsets) with the amounts of removed data are shown in Figure \ref{fig_transcribedNoiseHist2000} and \ref{fig_transcribedNoiseHist4000}.\\\\
\textbf{Objective evaluation}
\vspace{0.28cm}\\
Objective evaluation results are shown in table \ref{tab_werTranscribedNoise2000} and \ref{tab_werTranscribedNoise4000}. As can be seen from the tables, the voices synthesized after removing high transcribed noise recordings were more intelligible than the baseline ones.\\\\
\textbf{Subjective evaluation}
\vspace{0.28cm}\\
In subjective evaluation, two pairs of voices were evaluated. The first one coming from the 2000-recording dataset consisted of the baseline and the voice synthesized after removing 10\% (the best voice chosen from objective evaluation). The second one coming from the 4000-recording dataset consisted of the baseline and the voice synthesized after removing 20\% (the best voice chosen from objective evaluation). Subjective evaluation results from 27 evaluators are shown in Table \ref{tab_subEvaTranscribedNoise}.

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/werHist2000}
\end{center}
\vspace{-0.3cm}
\caption[transcribed noise Hist 2000.]{Histogram of transcribed noise in 2000-recording dataset.}
\label{fig_transcribedNoiseHist2000}
% \vspace{-0.3cm}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/werHist4000}
\end{center}
\vspace{-0.3cm}
\caption[transcribed noise Hist 4000.]{Histogram of transcribed noise in 4000-recording dataset.}
\label{fig_transcribedNoiseHist4000}
% \vspace{-0.3cm}
\end{figure}

\begin{table}[]
\begin{center}
\caption{WER of voices synthesized from 2000-recording dataset before and after removing highest transcribed noise recordings.}
\label{tab_werTranscribedNoise2000}
\vspace{3mm}
\begin{tabular}{lccc}
\hline
Voice & WER – Watson (\%) & WER – witai (\%) & WER – average (\%)\\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}\cmidrule(l{0pt}r{2pt}){3-3}\cmidrule(l{0pt}r{2pt}){4-4}
Baseline          & 68.15 & 82.08 & 75.12 \\
After remove 5\%  & 70.95 & 75.57 & 73.26 \\
After remove 10\% & 59.56 & 66.74 & 63.15 \\
After remove 15\% & 65.68 & 75.12 & 70.40 \\
After remove 20\% & 62.37 & 77.16 & 69.77 \\
After remove 25\% & 71.63 & 77.22 & 74.43 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[]
\begin{center}
\caption{WER of voices synthesized from 4000-recording dataset before and after removing highest transcribed noise recordings.}
\label{tab_werTranscribedNoise4000}
\vspace{3mm}
\begin{tabular}{lccc}
\hline
Voice & WER – Watson (\%) & WER – witai (\%) & WER – average (\%)\\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}\cmidrule(l{0pt}r{2pt}){3-3}\cmidrule(l{0pt}r{2pt}){4-4}
Baseline          & 67.77 & 78.81 & 73.29 \\
After remove 5\%  & 55.60 & 75.83 & 65.72 \\
After remove 10\% & 59.94 & 78.41 & 69.18 \\
After remove 15\% & 57.59 & 74.10 & 65.85 \\
After remove 20\% & 55.37 & 72.86 & 64.12 \\
After remove 25\% & 61.45 & 73.79 & 67.62 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[]
\begin{center}
\caption{Preference score of voices synthesized from 2000- and 4000-recording datasets before and after removing high transcribed noise recordings.}
\label{tab_subEvaTranscribedNoise}
\vspace{3mm}
\begin{tabular}{lcc}
\hline
Voice & 2000-recording dataset & 4000-recording dataset \\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}\cmidrule(l{0pt}r{2pt}){3-3}
Baseline & 22.22\% & 31.48\% \\
After removing high stdF0 recordings & 45.68\% & 37.96\% \\
No preference & 32.10\% & 30.56\% \\
\hline
\end{tabular}
\end{center}
\end{table}

Subjective evaluation results show improvements in quality for the voices synthesized after removing high transcribed noise recordings.

Combining objective and subjective evaluation results, we conclude that removing recordings with high transcribed noise does help improve the quality of synthetic voices.

\subsection{Summary of the 4 investigations}
In the previous section, we investigated the effect of 4 factors which are speech power, SNR, stdF0 and transcribed noise on the quality of voices synthesized from crowdsourced datasets. These 4 investigations are summarized in table \ref{tab_investigationsSummary}.

\begin{table}[]
\begin{center}
\caption{Summary of investigations of the 4 factors.}
\label{tab_investigationsSummary}
\vspace{3mm}
\begin{tabular}{llc}
\hline
% Factor & Data processing & Effect on synthetic voices: improve or not? \\
\multicolumn{1}{l}{Factor} & Data processing & \begin{tabular}[c]{@{}c@{}}Effect on synthetic voices:\\ improve or not?\end{tabular} \\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}\cmidrule(l{0pt}r{2pt}){3-3}
Speech power & Normalizing power of all recordings & Yes \\
stdF0 & Removing high stdF0 recordings & No \\
SNR & Removing low SNR recordings & Yes \\
Transcribed noise & Removing high transcribed noise recordings & Yes \\
\hline
\end{tabular}
\end{center}
\end{table}

%-----------------------------------------------------------------
% EVALUATING THE QUALITY OF CROWDSOURCED DATASETS AND PERFORMING DATA CLEANING
%-----------------------------------------------------------------

\clearpage
\section{Evaluating the Quality of Crowdsourced Datasets and Performing Data Cleaning}\label{sec_evaluateAndDataCleaning}
In this section, we focus on evaluating the quality of a crowdsourced speech dataset using the 3 effective factors, namely speech power, SNR and transcribed noise.

The investigations have proved the effectiveness of the 3 factors, however, in cases of SNR and transcribed noise, we would encounter confusion when dealing with how much data should be removed (data cleaning) to make the new dataset produce the best voice because as can be seen from the objective evaluation results, the amount of data should be removed to produce the best voice depends on the size of dataset. Our research aims to evaluate the quality of an arbitrary dataset, so we continue conducting 2 investigations for SNR and transcribed noise aiming to determine the thresholds of SNR and WER to remove bad data of each type.

\subsection{Determining threshold of SNR for data quality evaluation}
The idea of this investigation is trying removing data using many different SNR thresholds and then evaluate the quality of voices synthesized from the new datasets to see which threshold gives the best performance.

The investigation is conducted on 2 datasets which are, again, 2 subsets of 2000 and 4000 recordings extracted from Common Voice dataset. In each dataset, the following process is applied. The process begins with calculating SNR in each recording. The recordings are then sorted by their SNR values. Recordings with lowest SNR values are removed from the dataset using 8 different thresholds which are 10 dB, 15 dB, 20 dB, …, 45 dB. 8 voices corresponding to 8 new datasets are synthesized. Objective evaluation is then conducted to evaluate the quality of the voices by using 2 speech recognition APIs. WERs of the voices are shown in figure \ref{fig_snrThresh2000} and \ref{fig_snrThresh4000}.

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/snrThresh2000}
\end{center}
\vspace{-0.3cm}
\caption[SNR thresholds 2000.]{Word error rates of voices synthesized from 2000-recording dataset before and after removing data based on different SNR thresholds.}
\label{fig_snrThresh2000}
% \vspace{-0.3cm}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/snrThresh4000}
\end{center}
\vspace{-0.3cm}
\caption[SNR thresholds 4000.]{Word error rates of voices synthesized from 4000-recording dataset before and after removing data based on different SNR thresholds.}
\label{fig_snrThresh4000}
% \vspace{-0.3cm}
\end{figure}

We can see that, in both datasets, the trends of WERs are fairly similar and removing data using SNR threshold of 30 dB give the best voices. Therefore, 30 dB can be used as a threshold to remove low SNR recordings in a dataset.

\subsection{Determining threshold of WER for data quality evaluation}
As with the investigation of determining SNR threshold, we use the same approach for this investigation. We again try removing data using many different WER thresholds and then evaluate the quality of voices synthesized from the new datasets to see which threshold gives the best performance.

WER of a recording is calculated as the difference between its transcript and the text actually spoken. High WER means the transcript is incorrect or the speaker had poor pronunciation or they simply did not exactly speak what is in the trancript. What is spoken in the recording is obtained by using a speech recognition API (Watson API). After being recognized, the recognized text is compared to the transcript to calculate WER.

The investigation is conducted on 2 datasets, a 2000-recording dataset and a 4000-recording dataset. In each dataset, recordings with highest WER are removed using 8 different WER thresholds which are 90\%, 80\%, …, 20\%. 8 voices synthesized from 8 new datasets together with the original one are then evaluated by using 2 speech recognition APIs. Results of the objective evaluation are shown in figures \ref{fig_werThresh2000} and \ref{fig_werThresh4000}.

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/werThresh2000}
\end{center}
\vspace{-0.3cm}
\caption[WER thresholds 2000.]{Word error rates of voices synthesized from 2000-recording dataset before and after removing data based on different WER thresholds.}
\label{fig_werThresh2000}
% \vspace{-0.3cm}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/werThresh4000}
\end{center}
\vspace{-0.3cm}
\caption[WER thresholds 4000.]{Word error rates of voices synthesized from 4000-recording dataset before and after removing data based on different WER thresholds.}
\label{fig_werThresh4000}
% \vspace{-0.3cm}
\end{figure}

We observe that in 2000-recording and 4000-recording dataset, the best thresholds were 60\% and 70\%, respectively. However, because the trend at 70\% - 60\% was not stable, we decide not to use any value in this range to be the threshold for WER (for example, 60\% in 2000-recording dataset gave the best voice but almost the worst one in 4000-recording dataset).  Instead, we decide to choose a value in the range of 40\% - 30\% to be the threshold because this range show a more stable trend. Our final decision is to choose 35\% to be the threshold for WER when evaluating the quality of speech datasets based one transcribed noise factor.

\subsection{Evaluating the quality of crowdsourced datasets and performing data cleaning}
This section presents experiments of evaluating the quality of speech datasets and performing data cleaning using the 3 effective factors: speech power, SNR and transcribed noise. Voices synthesized before and after performing data cleaning are evaluated by both objective and subjective evaluations to see if data cleaning helps improve the quality of synthetic voices.

The order of evaluation and data cleaning is SNR $\rightarrow$ transcribed noise 	$\rightarrow$ speech power. The reason is that removing low SNR or noisy recordings from datasets can enhance the performance of speech recognizers on the remaining ones. Speech power comes last because after the step of removing noisy recordings, there are still slightly noisy recordings remain, so if the step of speech power is taken before the step of transcribed noise, power normalization can increase noise in these recordings which may degrade the performance of speech recognizers.

We use 2 datasets, a 2000-recording dataset and a 4000-recording dataset for the experiments

\subsubsection{2000-recording dataset}
The first step is to evaluate the dataset using SNR factor. Histogram of SNR in the dataset is shown in figure \ref{fig_snrHistCombination2000}.

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/snrHistCombination2000}
\end{center}
\vspace{-0.3cm}
\caption[SNR hist combination 2000.]{Histogram of SNR in 2000-recording dataset.}
\label{fig_snrHistCombination2000}
% \vspace{-0.3cm}
\end{figure}

From figure N, we can see that the dataset is quite clean but we can make it cleaner by removing recordings with SNRs lower than 30 dB.

After performing data cleaning using SNR factor, the remaining recordings are put into Watson API to recognize the text spoken in each recording. WER of each recording is calculated using its transcript and the recognized text. Histogram of WER is shown in figure \ref{fig_werHistCombination2000}. Recordings with WERs higher than 35\% are removed from the dataset.

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/werHistCombination2000}
\end{center}
\vspace{-0.3cm}
\caption[WER hist combination 2000.]{Histogram of WER in 2000-recording dataset after performing data cleaning using SNR factor.}
\label{fig_werHistCombination2000}
\end{figure}

The next step which is also the last step is to evaluate the dataset using speech power factor. Distribution of power in the dataset is shown in figure \ref{fig_powerDistributionCombination2000}.

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/powerDistributionCombination2000}
\end{center}
\vspace{-0.3cm}
\caption[power distribution combination 2000.]{Power distribution of 2000-recording dataset after performing data cleaning using transcribed noise factor in comparison with TTS datasets.}
\label{fig_powerDistributionCombination2000}
\end{figure}

We can see that the mean power of the dataset is acceptable compared to TTS datasets but the standard deviation is very much higher compared to the mean, so we normalized the power of all recordings in the dataset to the mean power. After performing power normalization, we get the final dataset of the data cleaning process. The voice synthesized from this dataset and the one synthesized from the original dataset are evaluated using objective and subjective evaluation.

Results of objective evaluation are shown in table \ref{tab_werCombination2000}.

\begin{table}[]
\begin{center}
\caption{WER of voices synthesized from 2000-recording dataset before and after performing data cleaning.}
\label{tab_werCombination2000}
\vspace{3mm}
\begin{tabular}{lccc}
\hline
Voice & WER – Watson (\%) & WER – witai (\%) & WER – average (\%)\\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}\cmidrule(l{0pt}r{2pt}){3-3}\cmidrule(l{0pt}r{2pt}){4-4}
Original            & 68.15 & 82.08 & 75.12 \\
After data cleaning & 58.36 & 72.42 & 65.39 \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{4000-recording dataset}
We also apply the same process of evaluating and cleaning data as for 2000-recording dataset. First, as can be seen from the histogram of SNR (figure \ref{fig_snrHistCombination4000}), the quality of the voice synthesized from the dataset can be improved by removing recordings with SNRs lower than 30 dB.

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/snrHistCombination4000}
\end{center}
\vspace{-0.3cm}
\caption[SNR hist combination 4000.]{Histogram of SNR in 4000-recording dataset.}
\label{fig_snrHistCombination4000}
\end{figure}

Histogram of WER in the dataset after removing low SNR recordings (figure \ref{fig_werHistCombination4000}) shows that the quality of the synthetic voice can be further improved by removing recordings with WER higher than 35\%.

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/werHistCombination4000}
\end{center}
\vspace{-0.3cm}
\caption[WER hist combination 4000.]{Histogram of WER in 4000-recording dataset after performing data cleaning using SNR factor.}
\label{fig_werHistCombination4000}
\end{figure}

The new dataset has a normal mean power but a very high power standard deviation (figure \ref{fig_powerDistributionCombination4000}). It means that the synthetic voice can be improved one more time by normalizing power of all recordings in the dataset.

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{image/powerDistributionCombination4000}
\end{center}
\vspace{-0.3cm}
\caption[power distribution combination 4000.]{Power distribution of 4000-recording dataset after performing data cleaning using transcribed noise factor in comparison with TTS datasets.}
\label{fig_powerDistributionCombination4000}
\end{figure}

Voice synthesized from the final dataset of the data cleaning process and the original voice are evaluate by objective and subjective evaluation. Results of objective evaluation are shown in table \ref{tab_werCombination4000}.

\begin{table}[]
\begin{center}
\caption{WER of voices synthesized from 4000-recording dataset before and after performing data cleaning.}
\label{tab_werCombination4000}
\vspace{3mm}
\begin{tabular}{lccc}
\hline
Voice & WER – Watson (\%) & WER – witai (\%) & WER – average (\%)\\
\cmidrule(l{0pt}r{2pt}){1-1}\cmidrule(l{0pt}r{2pt}){2-2}\cmidrule(l{0pt}r{2pt}){3-3}\cmidrule(l{0pt}r{2pt}){4-4}
Original            & 67.77 & 78.81 & 73.29 \\
After data cleaning & 53.42 & 75.93 & 64.68 \\
\hline
\end{tabular}
\end{center}
\end{table}





%-----------------------------------------------------------------
% CONCLUTION
%-----------------------------------------------------------------
\newpage
\section{Conclusion}\label{sec_conclusion}
We have investigated factors that might help improve the quality of voices synthesized from crowdsourced datasets, from that we introduced appropriate policies to process the data before training voices. The results showed that, three factors: speech power, signal-to-noise ratio (SNR) and transcribed noise do affect the quality of synthetic voices. We also determined the thresholds to remove bad data of each factor, namely 30 dB for SNR and 35\% for word error rate (WER) in case of transcribed noise. This means that recordings with SNR values lower than 30 dB or WER values higher than 35\% should be removed from the dataset. In addition, we found that with datasets that have high standard deviation of power, normalizing power of all recordings to the mean power helps improve the quality of synthetic voices. Results from experiments showed that, evaluating and performing data cleaning using the three factors gave significant improvements in quality of synthetic voices.

In the future, we will try to investigate some more factors that we hypothesize might affect the quality of voices synthesized from crowdsourced datasets such as articulation (computed as mean energy divided by speaking rate) and fluency (internal silences).

%-----------------------------------------------------------------
% APPENDIX
%-----------------------------------------------------------------
\clearpage
\appendix
\setcounter{secnumdepth}{0}
\section{List of sentences used to synthesize utterances in evaluating synthetic voices}\label{sec_appendix}

\subsection{Novel}
1.	And through another winter they wandered on the obliterated trails of men who had gone before.\\
2.	As the fall of the year came on, the moose appeared in greater abundance, moving slowly down to meet the winter in the lower and less rigorous valleys.\\
3.	The house lay on top of a small hill about one hundred feet higher than the barn and stables.\\
4.	He turned and saw her, and memories of his own terrible misery at school came flooding back to him.\\
5.	A bald man in a very long purple coat had actually shaken his hand in the street the other day and then walked away without a word.

\subsection{News}
1.	Opposition swells among those who are more familiar with what is happening in the region.\\
2.	Many of those surveyed express uncertainty about what to think; only half say they are very or somewhat familiar with rapidly changing developments in the region.\\
3.	The new offering currently features over fifty thousand grocery items and is available in three cities with plans to expand to more in the coming months.\\
4.	In twenty eighteen, income inequality in the US reached its highest level in more than half a century.\\
5.	Others have seen this coming as well, with some investors demanding discounts on coastal properties due to the anticipated effects of climate change.

\subsection{Conversation}
1.	Thank you for the drawing. I know where to put it.\\
2.	We have to go. The rain is coming.\\
3.	This is absolutely the best burger I have ever had.\\
4.	This is my younger sister. Do you think that we look alike?\\
5.	Could you please show me the way to the nearest station?
Scientific papers

\subsection{Scientific papers}
1.	An organized evaluation, based on listening tests, was then carried out to try to rank the systems and help identify the effectiveness of the techniques.\\
2.	In this paper, we present evaluations of different filtering techniques at the utterance level and their effect on TTS intelligibility.\\
3.	The subdivision of general outcome measurements leads to a greater differentiation, which provides a more realistic approach to gender differentiation in health and illness.\\
4.	In the present data, both genders exhibited a trend of impaired symptom reporting in older age groups.\\
5.	The mail survey was the most popular method for collecting research data on environmental issues, closely followed by personal interviews.

\newpage
%-------end--------------
\acknowledgements
This thesis is a summary of the research results conducted in the.
\par
First, I am grateful to my supervisor, Prof. Takanobu Nishiura,.
\par
I am grateful to members of my research group, and other members in Acoustics \& Signal Processing Labs (ASPL), for advising on my research and daily life. With their patient and enthusiastic help, I felt little difficult with research and study in English as a foreign student.
\par
At the end of this acknowledge, though it is impossible to list up everyone who has helped me here, I want to express my appreciation to my friends and family for their support.
\newpage
%-----------------------------------------------------------------
\bibliography{reference}
\end{document}
